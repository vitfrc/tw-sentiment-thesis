{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import string\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from emot.emo_unicode import EMOJI_UNICODE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# keys e tokens dalla Twitter Dev Console\n",
    "consumer_key = 'HErcO0a3RQxeBc4Au44qZZknQ'\n",
    "consumer_secret = 'PsC0EhWB5v9DUFVONHIMTpwRJAf211b6Ga77EFdDE9wV2Xah3S'\n",
    "access_token = '2656923476-BHCZUQTee9C5KufeqFKeQC1rw91YD4O7Xx6CUxi'\n",
    "access_token_secret = 'uNk9Ue5yCFmn0TxvfrrGxBOO7Z9q4QoAmn346JNS46Sby'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prova di autenticazione\n",
    "try:\n",
    "\t# creazione dell'oggetto OAuthHandler\n",
    "\tauth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\t# impostazione dell'access token e del secret\n",
    "\tauth.set_access_token(access_token, access_token_secret)\n",
    "\t# creazione dell'oggetto API tweepy per il fetch dei tweet\n",
    "\tapi = tweepy.API(auth)\n",
    "except:\n",
    "\tprint(\"Error: Authentication Failed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(query, count = 10):\n",
    "\t\n",
    "\t# lista vuota per conservare i tweet\n",
    "\ttweets_list = [tweets for tweets in tweepy.Cursor(api.search, q=query, lang=\"en\", tweet_mode='extended').items(count)]\n",
    "\n",
    "\t# scraping dei singoli tweet\n",
    "\twith open('tweets.csv', 'a', newline='', encoding='utf-8') as csvFile:\n",
    "\t\tcolumn_names = ['tweet_id', 'text', 'retweet', 'likes']\n",
    "\t\tcsv_writer = csv.DictWriter(csvFile, fieldnames=column_names, delimiter=',')\n",
    "\t\tcsv_writer.writeheader()\n",
    "\t\tfor tweet in tweets_list[::-1]:\n",
    "\t\t\tcsv_data = {}\n",
    "\t\t\tcsv_data['tweet_id'] = tweet.id\n",
    "\t\t\tcsv_data['text'] = tweet.full_text\n",
    "\t\t\tcsv_data['retweet'] = tweet.retweet_count\n",
    "\t\t\tcsv_data['likes'] = tweet.favorite_count\n",
    "\t\t\tcsv_writer.writerow(csv_data)\n",
    "\n",
    "get_tweets(query= 'facebook', count = 20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = []\n",
    "\n",
    "df = pd.read_csv('tweets.csv', index_col=None, header=0)\n",
    "tweets.append(df)\n",
    "\n",
    "tweets_df = pd.concat(tweets, axis=0, ignore_index=True)\n",
    "\n",
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.duplicated(subset='tweet_id').sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df = tweets_df.drop_duplicates(subset=['tweet_id'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.isna().any()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_words = stop_words + alphabets\n",
    "word_list = words.words()\n",
    "emojis = list(EMOJI_UNICODE.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_tweets(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\d+', '', tweet)\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "\n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emojis]\n",
    "    filtered_words = [w for w in filtered_words if w in word_list]\n",
    "\n",
    "    unpunctuated_words = [char for char in filtered_words if char not in string.punctuation]\n",
    "    unpunctuated_words = ' '.join(unpunctuated_words)\n",
    "\n",
    "    return \"\".join(unpunctuated_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_adjectives(tweet):\n",
    "    tweet = word_tokenize(tweet)\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet)\n",
    "        if tag == \"JJ\"]\n",
    "    return \" \".join(tweet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df['processed tweets'] = tweets_df['text'].apply(process_tweets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df['tweets adjectives'] = tweets_df['processed tweets'].apply(get_adjectives)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def lemmatize_tweets(tweet):\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "    return \" \".join(lemma_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df['lemmatized tweets'] = tweets_df['processed tweets'].apply(lemmatize_tweets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.to_csv('processed_tweets.csv', encoding='utf-8-sig', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_polarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "def get_sentiment(polarity):\n",
    "    if polarity < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df['polarity'] = tweets_df['lemmatized tweets'].apply(get_polarity)\n",
    "tweets_df['sentiment'] = tweets_df['polarity'].apply(get_sentiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentiment_chart = tweets_df['sentiment'].value_counts().rename_axis('sentiment').to_frame('total tweets').reset_index()\n",
    "sentiment_chart"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importazione delle librerie"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import string\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from emot.emo_unicode import EMOJI_UNICODE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tweets mining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# keys e tokens dalla Twitter Dev Console\n",
    "consumer_key = 'HErcO0a3RQxeBc4Au44qZZknQ'\n",
    "consumer_secret = 'PsC0EhWB5v9DUFVONHIMTpwRJAf211b6Ga77EFdDE9wV2Xah3S'\n",
    "access_token = '2656923476-BHCZUQTee9C5KufeqFKeQC1rw91YD4O7Xx6CUxi'\n",
    "access_token_secret = 'uNk9Ue5yCFmn0TxvfrrGxBOO7Z9q4QoAmn346JNS46Sby'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prova di autenticazione\n",
    "try:\n",
    "\t# creazione dell'oggetto OAuthHandler\n",
    "\tauth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\t# impostazione dell'access token e del secret\n",
    "\tauth.set_access_token(access_token, access_token_secret)\n",
    "\t# creazione dell'oggetto API tweepy per il fetch dei tweet\n",
    "\tapi = tweepy.API(auth)\n",
    "except:\n",
    "\tprint(\"Error: Authentication Failed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creazione delle funzione necessaria al mining dei tweet\n",
    "\n",
    "In questa sezione viene creata la funzione necessaria ad estrarre i tweet da Twitter mediante l'oggetto Cursor di Tweepy. In seguito, i tweet e i parametri a loro associati vengono aggiunti nel file \"tweets.csv\" per una gestione più modulare."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(query, count = 10):\n",
    "\t\n",
    "\t# lista vuota per conservare i tweet\n",
    "\ttweets_list = [tweets for tweets in tweepy.Cursor(api.search, q=query, lang=\"en\", tweet_mode='extended').items(count)]\n",
    "\n",
    "\t# scraping dei singoli tweet e scrittura del file tweets.csv\n",
    "\twith open('tweets.csv', 'a', newline='', encoding='utf-8') as csvFile:\n",
    "\t\tcolumn_names = ['tweet_id', 'text', 'retweet', 'likes']\n",
    "\t\tcsv_writer = csv.DictWriter(csvFile, fieldnames=column_names, delimiter=',')\n",
    "\t\tcsv_writer.writeheader()\n",
    "\t\tfor tweet in tweets_list[::-1]:\n",
    "\t\t\tcsv_data = {}\n",
    "\t\t\tcsv_data['tweet_id'] = tweet.id\n",
    "\t\t\tcsv_data['text'] = tweet.full_text\n",
    "\t\t\tcsv_data['retweet'] = tweet.retweet_count\n",
    "\t\t\tcsv_data['likes'] = tweet.favorite_count\n",
    "\t\t\tcsv_writer.writerow(csv_data)\n",
    "\n",
    "# utilizzo della funzione per estrarre i tweet secondo la query in questione\n",
    "get_tweets(query= 'facebook', count = 20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilizzo del Pandas Dataframe per gestire i tweet estratti all'interno del progetto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = []\n",
    "\n",
    "# creazione del DataFrame dal file csv\n",
    "df = pd.read_csv('tweets.csv', index_col=None, header=0)\n",
    "tweets.append(df)\n",
    "\n",
    "tweets_df = pd.concat(tweets, axis=0, ignore_index=True)\n",
    "\n",
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Controlli e ottimizzazione del Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.shape #controllo delle dimensioni del dataframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.duplicated(subset='tweet_id').sum() # controllo su valori duplicati (in base all'id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df = tweets_df.drop_duplicates(subset=['tweet_id']) # eliminazione di valori duplicati"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.shape # nuovo controllo delle dimensioni"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Processazione dei tweet\n",
    "\n",
    "Il testo dei tweet contiene molti elementi fuorvianti e inutili in fase di analisi. Qui sotto ci sono alcune funzioni necessarie a ripulirli e ad estrarre solo parti interessanti da analizzare successivamente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definizione delle variabili dai moduli nltk necessari alla processazione\n",
    "stop_words = list(stopwords.words('english'))\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_words = stop_words + alphabets\n",
    "word_list = words.words()\n",
    "emojis = list(EMOJI_UNICODE.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definizione della funzione necessaria ad eliminare link, caratteri speciali, stopwords,\n",
    "# emoji, punteggiatura dal testo del tweet e a tokenizzarlo\n",
    "def process_tweets(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\d+', '', tweet)\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "\n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emojis]\n",
    "    filtered_words = [w for w in filtered_words if w in word_list]\n",
    "\n",
    "    no_punc_words = [char for char in filtered_words if char not in string.punctuation]\n",
    "    no_punc_words = ' '.join(no_punc_words)\n",
    "\n",
    "    return \"\".join(no_punc_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definizione della funzione necessaria ad estrarre gli aggettivi dal testo del tweet\n",
    "def get_adjectives(tweet):\n",
    "    tweet = word_tokenize(tweet)\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet)\n",
    "        if tag == \"JJ\"]\n",
    "    return \" \".join(tweet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aggiunta colonna tweet processati al DataFrame\n",
    "tweets_df['processed tweets'] = tweets_df['text'].apply(process_tweets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aggiunta colonna aggettivi al DataFrame\n",
    "tweets_df['tweets adjectives'] = tweets_df['processed tweets'].apply(get_adjectives)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definizione della funzione che usa il lemmatizer per riportare le parole dei tweet alla loro forma base\n",
    "def lemmatize_tweets(tweet):\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "    return \" \".join(lemma_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aggiunta colonna tweet 'lemmatizzati' al DataFrame\n",
    "tweets_df['lemmatized tweets'] = tweets_df['processed tweets'].apply(lemmatize_tweets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In questa fase viene fatta la sentiment analysis vera e propria tramite TextBlob, che sfrutta il machine learning per determinare la polarità dei tweet forniti su una scala da -1 (negativi) a 1 (positivi), passando per 0 (neutrali)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definizione delle funzioni necessarie a calcolare la polarità e il sentiment dei tweet da TextBlob\n",
    "def get_polarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "def get_sentiment(polarity):\n",
    "    if polarity < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aggiunta colonne polarità e sentiment al DataFrame\n",
    "tweets_df['polarity'] = tweets_df['lemmatized tweets'].apply(get_polarity)\n",
    "tweets_df['sentiment'] = tweets_df['polarity'].apply(get_sentiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DataFrame per il conteggio dei tweet positivi, neutrali e negativi\n",
    "sentiment = tweets_df['sentiment'].value_counts().rename_axis('sentiment').to_frame('total tweets').reset_index()\n",
    "sentiment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creazione del grafico della distribuzione dei tweet positivi, neutrali e negativi\n",
    "sentiment_barchart = px.bar(sentiment, x = 'sentiment', y='total tweets', color='sentiment')\n",
    "\n",
    "sentiment_barchart.update_layout(title='Distribution of Sentiments Results',\n",
    "                                  margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0})\n",
    "\n",
    "sentiment_barchart.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creazione stringa con gli aggettivi estratti dai tweet\n",
    "tweets_string = tweets_df['tweets adjectives'].tolist()\n",
    "tweets_string = \" \".join(tweets_string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creazione della WordCloud contenente gli aggettivi più utilizzati\n",
    "wc = WordCloud(background_color='white', max_words=1500)\n",
    "\n",
    "# generazione della WordCloud\n",
    "wc.generate(tweets_string)\n",
    "\n",
    "# impostazioni per mostrare la WordCloud\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)  # impostazione larghezza\n",
    "fig.set_figheight(20)  # impostazione altezza\n",
    "\n",
    "plt.imshow(wc.recolor(random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creazione lista con gli aggettivi più utilizzati\n",
    "tweets_string = tweets_df['tweets adjectives'].tolist()\n",
    "tweets_list=[]\n",
    "for item in tweets_string:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        tweets_list.append(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uso del modulo collections per contare gli aggettivi\n",
    "from collections import Counter\n",
    "counts = Counter(tweets_list)\n",
    "\n",
    "# creazione del DataFrame con aggettivo e numero di occorrenze ordinato per numero di occorrenze discendente\n",
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df.columns = ['words', 'count']\n",
    "df.sort_values(by='count', ascending=False, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}